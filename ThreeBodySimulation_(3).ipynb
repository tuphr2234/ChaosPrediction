{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuphr2234/ChaosPredition/blob/main/ThreeBodySimulation_(3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TDAWMjwIKbl"
      },
      "source": [
        "Build Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h_D93EjYjFie"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from pathlib import Path  \n",
        "\n",
        "\n",
        "G=1\n",
        "\n",
        "\n",
        "class Body:\n",
        "  \n",
        "  prevPosition=np.array([0,0])\n",
        "  position=np.array([0,0])\n",
        "  velocity=np.array([0,0])\n",
        "  force=np.array([0,0])\n",
        "  acceleration=np.array([0,0])\n",
        "  mass=0\n",
        "\n",
        "  def __init__(self,position,velocity,mass,name):\n",
        "    self.position=np.array(position)\n",
        "    self.velocity=np.array(velocity)\n",
        "    #self.acceleration=np.array(acceleration)\n",
        "    self.mass=mass\n",
        "    self.prevPosition=np.array(position)\n",
        "    self.name = name\n",
        "\n",
        "\n",
        "  def setPosition(self,t):\n",
        "    self.prevPosition = self.position\n",
        "    self.position=self.position+self.velocity*t+(self.acceleration/2)* t**2\n",
        "  \n",
        "    \n",
        "\n",
        "\n",
        "  def setVelocity(self,t):\n",
        "    self.velocity=self.velocity+self.acceleration*t\n",
        "  \n",
        "\n",
        "  def getForce(self,body):\n",
        "    softening = 0.00001\n",
        "    Distance = body.prevPosition - self.position\n",
        "    Rsquare = Distance[0]**2 + Distance[1]**2 + softening\n",
        "    F = G* self.mass*body.mass / Rsquare  \n",
        "    normalizeDis = Distance / math.sqrt(Rsquare)\n",
        "    F = F*normalizeDis\n",
        "    \n",
        "    #print(f\"body {self.name} force is {F} \")\n",
        "    \n",
        "    return F\n",
        "\n",
        "  def setAcceleration(self,bodies):\n",
        "    F=np.array([0,0])\n",
        "    for body in bodies:\n",
        "      F=F+self.getForce(body)\n",
        "    self.acceleration = F/self.mass\n",
        "   # print(f\" {self.name} acceleration is {self.acceleration}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3N3yCUu4PPDX"
      },
      "outputs": [],
      "source": [
        "def simulateOnTimeStep(bodies,t,steps,interval_in_data): # bodies: list of bodies. t: timestep . steps: number of steps. interval_in_data : frequency of rows taken to dataset: if equals 1 than it is every step\n",
        "  dataset = []\n",
        "  time = []\n",
        "\n",
        "  for i in range(steps):\n",
        "      list1=[]\n",
        "      for body in bodies:\n",
        "        list1.append(body.position)\n",
        "      if i % interval_in_data ==0:\n",
        "        dataset += list1\n",
        "        time.append(i * t)\n",
        "      \n",
        "      for i,body in enumerate(bodies):\n",
        "        body.setAcceleration([b for b in bodies if b is not body])\n",
        "        body.setVelocity(t)\n",
        "        body.setPosition(t)\n",
        "       \n",
        "        \n",
        "      for body in bodies:\n",
        "        #plt.plot([body.prevPosition[0],body.position[0]],[body.prevPosition[1],body.position[1]])\n",
        "        body.prevPosition = body.position\n",
        "  #plt.show()\n",
        "    \n",
        "\n",
        "  return np.array(dataset).reshape(steps//interval_in_data,len(bodies),2),time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def numpyToPandas(simulation,numOfBodies):\n",
        "  columns = []\n",
        "\n",
        "  for i in range(numOfBodies):\n",
        "    columns.append(f'Body{i+1} x')\n",
        "    columns.append(f'Body{i+1} y')\n",
        "  simulation_reshaped = simulation.reshape(len(simulation), numOfBodies*2 )\n",
        "\n",
        "  df = pd.DataFrame(simulation_reshaped, columns = columns)\n",
        "\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "uyz_2jL4t6QF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "q_Bo_YOp1wxq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFmLiPZMDQZ0"
      },
      "source": [
        "Add Filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eBPe3vx-DVyb"
      },
      "outputs": [],
      "source": [
        "#def addNoise(simulation):\n",
        "#def removeBody(simulation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr0DATa23lkK"
      },
      "source": [
        "Save Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "K3LKWQt83uFP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQlYUvwaISvu"
      },
      "source": [
        "Prepare Dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-NmBUcC3I5LO"
      },
      "outputs": [],
      "source": [
        "def prepareData(simulation,window_size_X):\n",
        "  X = []\n",
        "  y = []\n",
        "  for i in range(len(simulation)-window_size_X):\n",
        "    row = [bodies for bodies in simulation[i:i+window_size_X]]  ### adding time is an option\n",
        "    X.append(row)\n",
        "    label = simulation[i+window_size_X]\n",
        "    y.append(label)\n",
        "  return np.array(X) , np.array(y)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Tfy7w3HsJU5P"
      },
      "outputs": [],
      "source": [
        "def prepareData2(simulation,window_size_X,window_size_y):\n",
        "  X = []\n",
        "  y = []\n",
        "  for i in range(len(simulation)-window_size_X):\n",
        "    row = simulation[i:i+window_size_X]  ### adding time is an option\n",
        "    X.append(row)\n",
        "    label =  simulation[i+window_size_X:i+window_size_X + window_size_y]#######################################\n",
        "    y.append(label)\n",
        "\n",
        "  return np.array(X) , np.array(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_l71q3WAbOke"
      },
      "outputs": [],
      "source": [
        "def splitToTrainValidTest(dataset,testPercentage):\n",
        "  trainLen = round((len(dataset)*(1-testPercentage-0.05)))\n",
        "  valLen = round((len(dataset)*0.05))\n",
        "  train_set = dataset[:trainLen]\n",
        "  val_set = dataset[trainLen:trainLen+valLen]\n",
        "  test_set = dataset[trainLen+valLen:]\n",
        "  return train_set,val_set,test_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fNpP_KDhHKuQ"
      },
      "outputs": [],
      "source": [
        "def drawSimulation(simulation):\n",
        "  prev = simulation[0]\n",
        "  numOfBodies = len(simulation[0])\n",
        "  for i,sim in enumerate(simulation):\n",
        "    for j in range(0,len(sim),2):\n",
        "      plt.plot([prev[j],sim[j]],[prev[j+1],sim[j+1]])\n",
        "      prev[j] = sim[j]\n",
        "      prev[j+1] = sim[j+1]\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def showBodiesMovmentInGraph(simulation,trueForX):\n",
        " arr = []\n",
        " for i in range(0,len(simulation),2):\n",
        "    for sim in simulation:\n",
        "      arr.append(sim[i])\n",
        "\n",
        "\n",
        " plt.plot(arr)"
      ],
      "metadata": {
        "id": "J9HQSryLlGpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BntFiYXOd44"
      },
      "source": [
        "Preprocessing/Postprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EF80L_VWOhvb"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkwyKYUNOlJ5"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "t1xoIIT1QZb_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "def buildModelLSTM(window_size_X,numberOfBodies):\n",
        "  model = Sequential()\n",
        "  model.add(InputLayer((window_size_X,numberOfBodies*2)))#model.add(InputLayer((window_size_X,numberOfBodies*2)))\n",
        "  model.add(Conv1D(64, kernel_size=2))\n",
        "  #model.add(Dropout(0.2))\n",
        "  model.add(Conv1D(32, kernel_size=2))\n",
        "  #model.add(TimeDistributed(conv_1d))\n",
        "  model.add(Flatten())\n",
        "  #model.add(LSTM(32))\n",
        "  #model.add(Dropout(0.2))\n",
        "  model.add(Dense(16, 'relu'))\n",
        "  model.add(Dense(numberOfBodies*2, 'linear'))\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "def buildModelLSTM2(window_size_X,window_size_y,numberOfBodies):\n",
        "  model = Sequential()\n",
        "  model.add(InputLayer((window_size_X,numberOfBodies*2)))#model.add(InputLayer((window_size_X,numberOfBodies*2)))\n",
        "  model.add(Conv1D(64, kernel_size=2))\n",
        "  #model.add(Dropout(0.2))\n",
        "  model.add(Conv1D(32, kernel_size=2))\n",
        "  #model.add(TimeDistributed(conv_1d))\n",
        "  model.add(Flatten())\n",
        "  #model.add(LSTM(32))\n",
        "  #model.add(Dropout(0.2))\n",
        "  model.add(Dense(16, 'relu'))\n",
        "  model.add(Dense(numberOfBodies*2*window_size_y, 'linear'))\n",
        "  model.add(tf.keras.layers.Reshape((window_size_y, numberOfBodies*2)))\n",
        "  model.summary()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessSimulation(simulation):\n",
        "  shape = simulation[0].shape\n",
        "  newSimulation = np.append(simulation[1:],[np.zeros(shape)],axis=0) - simulation\n",
        "  newSimulation = newSimulation[:-1]\n",
        "  #for i,s in enumerate(newSimulation):\n",
        "  #  s = s / np.linalg.norm(simulation[i])\n",
        "  return newSimulation"
      ],
      "metadata": {
        "id": "bmAH9y73-g7K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5MBgcHbzDAbi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess(lastKnownElement , listOfPredictedChanges):\n",
        "  listOfPredictions = []\n",
        "  for pred in listOfPredictedChanges:\n",
        "    #newElement = (pred * np.linalg.norm(lastKnownElement)) + lastKnownElement\n",
        "    newElement = pred  + lastKnownElement\n",
        "    listOfPredictions.append(newElement)\n",
        "    lastKnownElement = newElement\n",
        "  return np.array(listOfPredictions)  \n"
      ],
      "metadata": {
        "id": "qbQToROHTnr0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHN2E_WZcMwI"
      },
      "source": [
        "Training Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#position,velocity,mass\n",
        "#%%timeit\n",
        "bodies=[]\n",
        "bodies.append(Body([-0.97000436, 0.24308753],[0.4662036850, 0.4323657300],1,\"body1\"))\n",
        "bodies.append(Body([0,0],[-0.93240737, -0.86473146],1,\"body2\"))\n",
        "bodies.append(Body([0.97000436, -0.24308753],[0.4662036850, 0.4323657300],1,\"body3\"))\n",
        "#bodies.append(Body([2, -0.24308753],[-0.4662036850, 0.4323657300],1,\"body4\"))\n",
        "\n",
        " #r1(0) = −r3(0) = (−0.97000436, 0.24308753);\n",
        " # r2(0) = (0,0); v1(0) = v3(0) = (0.4662036850, 0.4323657300);\n",
        " # v2(0) = (−0.93240737, −0.86473146). The values are obtained from Chenciner & Montgomery (2000).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "numOfBodies=len(bodies)\n",
        "\n",
        "##t: timestep . steps: number of steps. interval_in_data\n",
        "timestep = 0.001 # actual seconds\n",
        "steps = 122000\n",
        "interval_in_data = 10\n",
        "\n",
        "simulation1,timesteps = simulateOnTimeStep(bodies,timestep,steps,interval_in_data)\n",
        "simulation1=simulation1.reshape(len(simulation1), numOfBodies*2 )\n"
      ],
      "metadata": {
        "id": "EYCRyk7h_AGq"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drawSimulation(simulation1)"
      ],
      "metadata": {
        "id": "sku5JwPo4KxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bodies2=[]\n",
        "bodies2.append(Body([-1, 0],[0.6976850, 0.4323657300],1,\"body1\"))\n",
        "bodies2.append(Body([0,1],[0.23540737, -0.86473146],1.1,\"body2\"))\n",
        "bodies2.append(Body([1, 0],[-0.4662036850, -0.1823657300],1,\"body3\"))\n",
        "#bodies.append(Body([2, -0.2308753],[0.4662036850, 0.4323657300],1,\"body4\"))\n",
        "\n",
        "numOfBodies2=len(bodies2)\n",
        "\n",
        "##t: timestep . steps: number of steps. interval_in_data\n",
        "timestep = 0.01 # actual seconds\n",
        "steps = 1000000\n",
        "interval_in_data = 100\n",
        "\n",
        "simulation2,timesteps = simulateOnTimeStep(bodies2,timestep,steps,interval_in_data)\n",
        "simulation2=simulation2.reshape(len(simulation2), numOfBodies2*2 )"
      ],
      "metadata": {
        "id": "LlfDZVNZSQKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8xnxXQz39xEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## save simulation as csv\n",
        "def saveSimulation(simulation,numOfBodies,simulation_name):\n",
        "  df = numpyToPandas(simulation,numOfBodies) \n",
        "  df.to_csv(f'/content/{simulation_name}.csv')"
      ],
      "metadata": {
        "id": "KKcQqBn28FpZ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saveSimulation(simulation1,numOfBodies,\"best of chaos\")"
      ],
      "metadata": {
        "id": "LEkOnA1VjuXj"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWvEtpy8YWDV"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.signal import hilbert\n",
        "from scipy.fft import fft, ifft\n",
        "scaled = False \n",
        "windowSizeX= 30\n",
        "windowSizeY = 10\n",
        "#batch = 6\n",
        "\n",
        "derivativeSimulation1 = preprocessSimulation(simulation1)\n",
        "\n",
        "train, val, test = splitToTrainValidTest(derivativeSimulation1,0.2)\n",
        "\n",
        "if scaled:\n",
        "  scaler = StandardScaler()\n",
        "  scaler.fit(simulation1[:len(train)+len(val)])\n",
        "  #valScaler.fit(train)  ### train / val as input ? \n",
        "\n",
        "  train = scaler.transform(train)\n",
        "  val = scaler.transform(val)\n",
        "  #train = hilbert(train)\n",
        "  #val = hilbert(val)\n",
        "#y_train = scaler.fit(train)\n",
        "\n",
        "#train = preprocessSimulation(train)\n",
        "#val = preprocessSimulation(val)\n",
        "\n",
        "\n",
        "\n",
        "#train2, val2, test2 = splitToTrainValidTest(simulation2,0.2)\n",
        "\n",
        "\n",
        "X_train, y_train = prepareData(train,windowSizeX)\n",
        "X_val, y_val = prepareData(val,windowSizeX)\n",
        "X_test, y_test = prepareData(test,windowSizeX)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model =  buildModelLSTM(windowSizeX,numOfBodies)\n",
        "cp1 = ModelCheckpoint('model/', save_best_only=True)\n",
        "model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.0001), metrics=[RootMeanSquaredError()])\n",
        "\n",
        "\n",
        "#X_train2, y_train2 = prepareData(train2,windowSizeX)\n",
        "#X_val2, y_val2 = prepareData(val2,windowSizeX)\n",
        "#X_tes2t, y_test2 = prepareData(test2,windowSizeX)\n",
        "\n",
        "#generator = TimeseriesGenerator(train_scaled,train_scaled,length = len(test),batch_size=batch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7LEw5INYMMTx"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Y4yPGB9XzsX2"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfb7-Rz2-IrH"
      },
      "outputs": [],
      "source": [
        "#for i in range(100):\n",
        "  #model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=2, callbacks=[cp1])\n",
        "  #model.fit(X_train2, y_train2, validation_data=(X_val2, y_val2), epochs=2, callbacks=[cp1])\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200, callbacks=[cp1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyBfk-DU1nfB"
      },
      "outputs": [],
      "source": [
        "#history = model.fit_generator(generator,epochs=100,verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhk15oVWvwCL"
      },
      "source": [
        "Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "uQHf2mwmvuPN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test)          ###############  LOCAL RMSE  (prediction per window)\n",
        "#predictions = postprocess(val[-1],predictions)\n",
        "#predictions = scaler.inverse_transform(predictions)\n",
        "print(math.sqrt(mean_squared_error(test[windowSizeX:],predictions)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RAT1QYFEZ6M",
        "outputId": "a0a6c790-f9db-4cf8-ba29-9d2d7393fd28"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.002013177611595624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "EnTNFFfIOkJC"
      },
      "outputs": [],
      "source": [
        "def slidingWindowActualPredictions(model,windowArr,windowSize,NumOfBodies,predictionsRequestedNumber):\n",
        "  \n",
        "\n",
        "  actualPredictionArr =[]\n",
        "  for i in range(predictionsRequestedNumber):\n",
        "\n",
        "    prediction = model.predict(windowArr[-1:]) ### predict from last window\n",
        "    actualPredictionArr.append(prediction)\n",
        "    temp = np.zeros([windowSize,numOfBodies*2])\n",
        "    temp = temp + windowArr[-1]\n",
        "    temp =temp[1:]\n",
        "    newWindow = np.append(temp.reshape(windowSize-1,numOfBodies*2),prediction)\n",
        "    newWindow = newWindow.reshape(windowSize,numOfBodies*2)\n",
        "    lengthX = len(windowArr)\n",
        "    windowArr = np.append(windowArr,newWindow) ### add new window that the last vector is the prediction \n",
        "    windowArr = windowArr.reshape(lengthX+1,windowSize,numOfBodies*2)\n",
        "\n",
        "  return actualPredictionArr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "1SFrwSvrMZ1d"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "listOfPredictions = slidingWindowActualPredictions(model,X_train,windowSizeX,numOfBodies,len(val)+len(test))\n",
        "listOfPredictions =np.array(listOfPredictions)\n",
        "listOfPredictions = listOfPredictions.squeeze()\n",
        "\n",
        "listOfPredictionsTrue = postprocess(simulation1[len(X_train)+windowSizeX-1],listOfPredictions)\n",
        "\n",
        "if scaled:\n",
        "  listOfPredictionsTrue = scaler.inverse_transform(listOfPredictionsTrue)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "listOfPredictions[1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrjhGOUvczUX",
        "outputId": "c7b68cce-ebdd-42f5-ac18-33a6d856de75"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([nan, nan, nan, nan, nan, nan], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#listOfPredictionsTrue = postprocess(simulation1[len(X_train)+windowSizeX-1],listOfPredictions)\n"
      ],
      "metadata": {
        "id": "ylve8dya6bzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "o3dEZ9WaZEF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "Eh4mEj6KwB6F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "6b9dc49d-0b27-406a-85ee-d13793c2ab82"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-176-e8a579c6cbca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulation1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mwindowSizeX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlistOfPredictionsTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m##### ACTUAL RMSE #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \"\"\"\n\u001b[1;32m    438\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     )\n\u001b[1;32m    441\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    114\u001b[0m             raise ValueError(\n\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[0;32m--> 116\u001b[0;31m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 )\n\u001b[1;32m    118\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
          ]
        }
      ],
      "source": [
        "print(math.sqrt(mean_squared_error(simulation1[len(X_train)+windowSizeX+1:],listOfPredictionsTrue))) ##### ACTUAL RMSE #####\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CiWTbSmryThx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnaGFCau-NRn"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLhJtfqxVcOm"
      },
      "outputs": [],
      "source": [
        "print((simulation1[len(X_train)+windowSizeX]))\n",
        "\n",
        "print((train[-1:]))\n",
        "#y_train =sc.inverse_transform(y_train)\n",
        "#print((y_train[-1:]))\n",
        "print(listOfPredictions[0])\n",
        "if scaled:\n",
        "  print((scaler.inverse_transform(model.predict(X_train[-1:]))))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "unkb0ud1ddUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rgzd9Tbv9kBn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "disArr = []\n",
        "for i in range(len(val)+len(test)):\n",
        "  disArr.append(np.linalg.norm(simulation1[len(X_train)+windowSizeX+i]-listOfPredictionsTrue[i]))\n",
        "\n",
        "plt.plot(disArr)  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drawSimulation(simulation1[len(X_train)+windowSizeX:len(X_train)+windowSizeX+60])"
      ],
      "metadata": {
        "id": "GOIM2jG_xrd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuehIDzY8P1I"
      },
      "outputs": [],
      "source": [
        "drawSimulation(listOfPredictionsTrue[:60])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simulation1.shape"
      ],
      "metadata": {
        "id": "28gEb8fR3nZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYecNFs6ur2T"
      },
      "outputs": [],
      "source": [
        "bodies=[]\n",
        "bodies.append(Body([-1, 0],[0.4662036860, 0.4323657300],1,\"body1\"))\n",
        "bodies.append(Body([0,0],[-0.93240737, -0.86473146],1.1,\"body2\"))\n",
        "bodies.append(Body([1, 0],[0.4662036850, 0.4323657300],1,\"body3\"))\n",
        "bodies.append(Body([2, -0.24308753],[-0.4662036850, -0.4323657300],1,\"body4\"))\n",
        "\n",
        "\n",
        "simulation2 = simulateOnTimeStep(bodies,0.01,100000,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5UUdwkIuw9E"
      },
      "outputs": [],
      "source": [
        "distance = simulation1 - simulation2\n",
        "curentDis=0\n",
        "disArr = []\n",
        "for i in range(100000):\n",
        "  for j in range(4):\n",
        "    curentDis = curentDis + math.sqrt(distance[i][j][0]**2 + distance[i][j][1]**2) \n",
        "  disArr.append(curentDis)\n",
        "  curentDis=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kay3tR8myegL"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6spMqcILZ5c"
      },
      "outputs": [],
      "source": [
        "time=[]\n",
        "for i in range(100000):\n",
        "  time.append(i*0.01)\n",
        "plt.plot(time[0:10000],disArr[0:10000])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLOve8ovQo6m"
      },
      "outputs": [],
      "source": [
        "disArr = np.array(disArr)\n",
        "shiftdisArr=disArr[1:]\n",
        "shiftdisArr=np.append(shiftdisArr,[0])\n",
        "len(shiftdisArr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1Zmm3WPA4lf"
      },
      "outputs": [],
      "source": [
        "disArrderiv = (shiftdisArr - disArr)/0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "comlUzy1Bt4A"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1bwq35R__4-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2BUe5tTU0mc"
      },
      "outputs": [],
      "source": [
        "time=[]\n",
        "for i in range(100000):\n",
        "  time.append(i*0.01)\n",
        "plt.plot(time[500:40000],disArrderiv[500:40000])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJPBvhY3QnLY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-oDRhtVpTpv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWHmnaXlL-UU"
      },
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mQ0NXGxQpMo"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrR93jf9jgDx"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ThreeBodySimulation (3).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}